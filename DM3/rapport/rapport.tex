\documentclass{article}
%\usepackage[latin1]{inputenc}
\usepackage{graphicx,amssymb,amsmath,amsbsy,MnSymbol} % extensions pour maths avancées
\usepackage{graphicx,mathenv}           % extensions pour figures
\usepackage[T1]{fontenc}        % pour les charactères accentués 
\usepackage[utf8]{inputenc} 

\usepackage{stmaryrd} % Pour les crochets d'ensemble d'entier
\usepackage{float}  % Pour placer les images là ou JE veux.

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\argmax}{argmax}


\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.3in}
\setlength{\topmargin}{-0.4in}
\setlength{\topskip}{1in}    % between header and text
\setlength{\textheight}{8in} % height of main text
\setlength{\textwidth}{6in}    % width of text
\setlength{\oddsidemargin}{0.5in} % odd page left margin
\setlength{\evensidemargin}{0.5in} % even page left margin
%
%% Quelques raccourcis clavier :
\def\slantfrac#1#2{\kern.1em^{#1}\kern-.3em/\kern-.1em_{#2}}
\def\b#1{\mathbf{#1}}
\def\bs#1{\boldsymbol{#1}}
\def\m#1{\mathrm{#1}}
%
\newcommand{\greeksym}[1]{{\usefont{U}{psy}{m}{n}#1}}
\newcommand{\inc}{\mbox{\small\greeksym{d}\hskip 0.05ex}}%
\pagenumbering{arabic}
\date{\today}
\title{DM3: Modèles Graphiques}
\author{Nelle Varoquaux}
\begin{document}
\maketitle
\tableofcontents{}
\vfill \eject

\section{Propagation de messages dans les modèles gaussiens}

Soit un modèle Gaussien non orienté de paramètre canonique $\eta \in
\Re$ et $\Lambda \in \Re^{n * n}$. On considère le cas où le modèle graphe est
un arche $G = (V, E)$.

\subsection{Propagation de messages sous forme d'exponentielle quadratique}

On sait que:

\begin{align}
p(x | \eta, \Lambda) & = & \exp\{a + \eta^Tx - \frac{1}{2} x^T \Lambda x\} \\
		     & = & \exp\{a + \sum_{i=1}^n \eta_{i} x_{i} - \sum_{i=1}^n \frac{1}{2} x_i \lambda_{ij} x_j\}
\end{align}

De plus,
\begin{equation}
p(x | \eta, \Lambda) = \frac{1}{Z} \prod_{i \in \mathcal{V}} \psi_i(x_i) \prod_{i,j \in \mathcal{E}} \psi_{i, j}(x_i, x_j)
\end{equation}

Sachant que $\lambda_{ij} = 0$ si $(i, j) \notin \mathcal{E}$, on peut en déduire que:

\begin{System}
\psi_i(x_i) = \exp( \eta_i x_i - \frac{1}{2} x_i ^2\lambda_{ii}) \\
\psi_{ij}(x_i, x_j) = \exp(- \lambda_{ij} x_i x_j)
\end{System}

On sait que les messages $\mu_{i \rightarrow j}$ sont définit par:

\begin{align}
\mu_{j \rightarrow i}(x_i) & = & \int_{x_j \in \Re} \psi_{ij}(x_i, x_j) \psi_j (x_j) \prod_{k \in \mathcal{C}_j} \mu_{k \rightarrow j}(x_j) \\
			   & = & \int_{x_j \in \Re} \exp(- \lambda_{ij} x_i x_j) \exp(\eta_j x_j - \frac{1}{2} x_j ^2\lambda_{jj}) \prod_{k \in \mathcal{C}_j} \mu_{k \rightarrow j}(x_j)
\end{align}

Commençons par étudier le cas des feuilles:

\begin{align}
\mu_{j \rightarrow i}(x_i) & = & \int_{\Re} \exp(- \lambda_{ij} x_i x_j) \exp( \eta_j x_j - \frac{1}{2} x_j ^2\lambda_{jj}) dx_j \\
			   & = & \int_{\Re} \exp \{ - \frac{1}{2} (\lambda_{jj}x_j^2 + 2x_j(\lambda_{ij}x_i - \eta_j) )\} dx_j \\
			   & = & \int_{\Re} \exp \{ - \frac{1}{2} (\sqrt{\lambda_{jj}}x_j + \frac{\lambda_{ij}x_i - \eta_j}{\sqrt{\lambda_{jj}}})^2 + \frac{(\lambda_{ij}x_i - \eta_j)^2}{2\lambda_{jj}}\} dx_j \\
			   & = & \exp(\frac{(\lambda_{ij}x_i - \eta_j)^2}{2\lambda_{jj}}) \int_{\Re} \exp(\frac{-x^2}{2}) \frac{1}{\sqrt{\lambda_{jj}}}dx \\
			   & = & 2 \sqrt{\frac{\pi}{2 \lambda_{jj}}}\exp(\frac{(\lambda_{ij}x_i - \eta_j)^2}{2\lambda_{jj}})
\end{align}

Nous obtenons donc bien une expression du message sous forme d'exponentielle quadratique.

Faisons l'hypothèse que les messages $\mu_{k \rightarrow j}$ peuvent se mettre
sous la forme $\mu_{k \rightarrow j}(x_j) = e^{\alpha_{k \rightarrow j} x_j^2 +
\beta_{k \rightarrow i} x_j^2  + \gamma_{k \rightarrow j}}$.

Nous avons déjà démontré que cela était vrai pour les racines. Faisons le maintenant pour le cas générale.
On a donc:
\begin{align}
\mu_{j \rightarrow i}(x_i) & = & \int_{x_j \in \Re} \exp(- \lambda_{ij} x_i x_j) \exp(\eta_j x_j - \frac{1}{2} x_j ^2\lambda_{jj}) \prod_{k \in \mathcal{C}_j} \mu_{k \rightarrow j}(x_j) \\
			   & = & \int_{x_j \in \Re} \exp(- \lambda_{ij} x_i x_j) + \eta_j x_j - \frac{1}{2} x_j ^2\lambda_{jj}) \prod_{k \in \mathcal{C}_j} e^{\alpha_{k \rightarrow j} x_j^2 + \beta_{k \rightarrow i} x_j^2  + \gamma_{k \rightarrow j}} dx_j \\
			   & = & \int_{x_j \in \Re} e^{a x_j^2 + b x_j  + c} dx_j
\end{align}

Dont le résultat peut s'exprimer sous la forme d'une exponentielle de quadratique.

\subsection{Calcul de $\mu$ et $\mathbb{E}[X_s, X_t]_{s, t \in E}$}

\section{Apprentissage de la structure d'un arbre}


\subsection{Entropie Marginale}

Soit une variable aléatoire discrète $X$ à valeurs dans un ensemble fini
$\mathcal{X}$. Soit $\eta(x) = p(X =x)$ le vecteur de paramètres. Soit un
échantillon iid $x^n), n=1, \dots, N$ de taille $N$ de cette variable.

On note $\hat{p}(x)$ la densité empirique définie par $\hat{p}(x) =
\frac{1}{N} \sum_{n=1}^N \delta(x^n = x)$ et $p_{\eta}(x)$ le maximum de vraisemblance.

On sait que:

\begin{equation}
\label{minKL-maxlog}
\min_{\eta} KL(\hat{p}|p_{\eta}) = \max_{\eta} \sum_{n = 1}^N \log p_{\eta}(x^n)
\end{equation}

Exprimons la divergence de Kullback-Liebler:

\begin{align}
KL(\hat{p}|p_{\eta}) & = & \sum_{x \in \mathcal{X}} \hat{p}(x) \log(\frac{\hat{p}(x)}{p_{\eta}(x)}) \\
		     & = & \sum_{x \in \mathcal{X}} \hat{p}(x) (\log(\hat{p}(x)) - \log(p_{\eta}(x))) \\
		     & = & - NH(x) - \sum_{x \in \mathcal{X}} \hat{p}(x) \log(p_{\eta}(x))
\end{align}

En utilisant \ref{minKL-maxlog}, on peut écrire:

\begin{align}
\max_{\eta} \sum_{n = 1}^N \log p_{\eta}(x^n) & = & \min_{\eta} \{- NH(x) - \sum_{x \in \mathcal{X}} \hat{p}(x) \log(p_{\eta}(x))\} \\
					      & = & - NH(x) - \min_{\eta} \{\sum_{x \in \mathcal{X}} \hat{p}(x) \log(p_{\eta}(x))\} \\
					      & = & - NH(x)
\end{align}

\subsection{Entropie jointe et conditionnelle}

Soit deux variables aléatoires discrètes $X$, $Y$ à valeurs dans les ensembles
finis $\mathcal{X}$ et $\mathcal{Y}$. Soit $\eta(x, y) = p(Y = y | X = x)$ la
matrice de paramètres de la loi conditionnelle. Soit un échantillon i.i.d.
$(x^n, y^n), n=1, \dots, N$ de taille $N$. On définit la densité empirique
$\hat{p}(x, y) = \frac{1}{N} \sum_{n=1}^N \delta(x^n = x) \delta(y^n = y)$.

On pose $q(x) = p(y | x)$

\begin{align}
KL(\hat{p | p_{\eta}} ) & = & \sum_{(x, y) \in (\mathcal{X}, \mathcal{Y})} \hat{p}(x, y) \log(\frac{\hat{p}(x, y)}{p_{\eta}(x, y)}) \\
			& = & \sum_{(x, y) \in (\mathcal{X}, \mathcal{Y})} \hat{p}(x, y) (\log(\hat{p}(x, y)) - \log(p_{\eta}(x, y))
\end{align}

\subsection{Question 3}

Considérons maintenant $P$ variables aléatoires $X_1, \dots, X_P$ à support
fini $\mathcal{X}_1, \dots, \mathcal{X}_P$. On considère $N$ observations
i.i.d. de ces $P$ variables, $(x_p^{n}), p = 1, \dots, P, n = 1, \dots, N$. On
note $\hat{p}(x_1, \dots, x_P)$ la densité empirique définie par:

\begin{equation}
\hat{p}(x_1, \dots, x_P) = \frac{1}{N} \sum_{n=1}^{N} \delta(x_1^n = x_1) \dots \delta(x_P^n = x_P)
\end{equation}

Soit un arbre orienté $T$ couvrant à $P$ sommets. Cette arbre a au plus un
parent par sommet. En notant $r$, la racine de l'arbre, et $\pi_i$ les voisins
de $i$, on peut donc exprimer la loi $p(x_1, \dots, x_P)$ comme:

\begin{equation}
p(x_1, \dots, x_P) = \prod_{p \in V} p(x_p | x_{\pi_p})
\end{equation}

Or, $\pi_i$ a au plus un élément. D'où:

\begin{equation}
p(x_1, \dots, x_P) = \prod_{p \in V}p(x_p | x_{p_j})
\end{equation}

où $x_{p_j}$ est l'unique parent de $x_p$ ou $\varnothing$ si celui-ci n'a pas
de parent.

En suivant cette convention de notation, sachant que l'on peut maximiser les
terms du maximum de vraisemblance indépendament les uns des autres:

\begin{align}
l(T) & = & \max\sum_{p \in V} \log p(x_p | x_{p_j}) \\
     & = & \sum_{p \in V} \max \log p(x_p | x_{p_j}) \\
     & = & \sum_{p \in V} N(H(X_p) - H(X_p, X_{\pi_p(T)}) \\
     & = & N \sum_{p \in V} H(X_p) - H(X_p, X_{\pi_p(T)})
\end{align}

\subsection{Question 4 - Information Mutuelle Empirique}

% TODO plus détaillé les calculs, entre autre au niveau des sommes sur x de p^(x, y) = p^(y)

\begin{align}
KL(\hat{p}(x, y) || \hat{p}(x)\hat{p}(y)) & = & \sum_{x, y} \hat{p}(x, y) \log \frac{\hat{p}(x, y)}{\hat{p}(x)\hat{p}(y)} \\
					  & = & \sum_{x, y} \hat{p}(x, y) \log \hat{p}(x, y) - \sum_{x, y} \hat{p}(x, y) \log \hat{p}(x) - \sum_{x, y} \hat{p}(x, y) \log \hat{p}(y) \\
					  & = & \sum_{x, y} \hat{p}(x, y) \log \hat{p}(x, y) - \sum_{x} \hat{p}(x) \log \hat{p}(x) - \sum_{y} \hat{p}(y) \log \hat{p}(y) \\
					  & = & -H(X, Y) + H(X) + H(Y) \\
					  & = & I(X || Y)
\end{align}


Montrons maitenant qu'une divergence de Kullback-Liebler est toujours positive ou nulle.

Soit $q$ et $p$.

\begin{align}
KL(p || q) & = & \sum_{\mathcal{X}} \{ \frac{p(x)}{q(x)} \log \frac{p(x)}{q(x)} \} q(x) \\
	   & = & \mathbb{E}_q [\frac{p(x)}{q(x)} \log \frac{p(x)}{q(x)}]
\end{align}


On pose $f: x \rightarrow x \log(x)$
$f$ est convexe. On peut donc utiliser Jensen:

\begin{align}
KL(p || q) & = & \mathbb{E}_q [f(\frac{p(x)}{q(x)})] \\
	   & \geq & f( \mathbb{E}_q[\frac{p(x)}{q(x)}] \\
	   & \geq & f(1) \\
	   & \geq & 0
\end{align}

\subsection{}

\begin{align}
l(T) & = & N \sum_{p=1}^N \{H_{X_{\pi(T)}} - H(X_p, X_{\pi_p(T)}\} \\
     & = & N \sum_{p=1}^N \{- H(X_p, X_{\pi_{p(T)}}) + H(X_{\pi_{p(T)}}) + H(X_p) - H(X_p)\} \\
     & = & N \sum_{p=1}^N I(X_p, X_{\pi_{p(T)}}) - N \sum_{p=1}^N H(X_p)
\end{align}


Le deuxième terme de la log vraisemblance est indépendante de la structure du
graphe. Pour maximiser $l(T)$ par rapport à l'arbre $T$, il suffit donc de
maximiser le premier terme du graphe. On remarque par ailleurs que cette
maximisation est indépendante de l'orientation du graphe: seule la présence
d'une arête importe.

On retrouve ici la problématique de l'arbre couvrant de poids maximal. Grâce à
l'information mutuelle entre deux points, calculable avec les densités, on peut
définir un coût entre chaque arête d'un arbre $G$, à $P$ sommets, et $P(P - 1)$
arêtes. Ces coûts permettent de trouver un arbre couvrant de poids maximal, à
savoir un arbre couvrant (ie, un arbre qui connecte tous les sommets du graphe
$G$ ensemble) dont le poids est supérieur ou égal à celui de tous les autres
graphes couvrants du graphe. L'arbre couvrant maximal est l'arbre ayant la
structure maximisant le maximum de vraisemblance.

Il existe différents algorithmes permettant de calculer un tel arbre, le plus
connu étant certainement l'algorithme de Kruskal (généralement utilisé pour
calculer l'arbre couvrant minimal). Cet algorithme consiste à ranger les
informations mutuelles par ordre décroissant, c'est-à-dire les arêtes par poids
décroissants, et d'ajouter les arêtes à l'arbre couvrant tant que cet ajout ne
fait pas apparaître de cycle. On obtient ainsi un arbre non orienté. On peut
ensuite choisir une racine, afin de l'orienter.


%% FIXME
La complexité d'un tel algorithme est $P(P-1)\log(P) + PN$. Il faut une
complexité de $PN$ pour calculer les informations mutelles, et $P(P-1)\log(P)$
pour l'algorithme de Kruskal.

\section{Implémentation - HMM}
\subsection{Equations d'estimation EM}

\begin{equation}
p(q, y, \theta) =  \prod_{i = 1}^4 \pi_i {q_0^i} \prod_{t = 0}^{T - 2} \prod_{i, j} a_{ij}^{q_t^i q_{t + 1}^j} \prod_{t = 0}^{T - 1} \prod_{i = 1}^{4} \mathcal{N}(\mu_i, \Sigma_i)(y_t)^{q_t^i}
\end{equation}

\begin{align}
l(q, y, \theta) & = & \sum_{i = 1}^4 q_0^i \ln \pi_i + \sum_{t = 0}^{T - 2} \sum_{i, j} q_t^i q_{t + 1}^j \ln a_{ij} + \sum_{t = 0}^{T - 1} \sum_{i = 1}^4 q_t^i \ln(\mathcal{N}(\mu_i, \Sigma_i)(y_t))
\end{align}

\begin{itemize}
\item En maximisant par $\pi_i$ \\
  \begin{equation}
  \pi_i^{(n + 1)} = \frac{q_0^i}{\sum_{j = 1}^4 q_0^j} = q_0^i
  \end{equation}

\item En maximisant par $a_{ij}$ \\
  \begin{equation}
  a_{ij}^{(n + 1)} = \frac{\sum_{t = 0}^{T - 2} q_t^i q_{t + 1}^j}{\sum_{j = 1}^4 \sum_{t = 0}^{T - 2}q_t^i q_{t + 1}^j}
  \end{equation}

\item En maximisant par $\mu_i$: \\
  le terme de $l(q, y, \theta)$ dépendant de $\mu_i$ et $\Sigma_i$ est le suivant:
  \begin{align}
  \sum_{i = 1}^{4} \sum_{t = 0}^{T - 1} q_t^i \{ - \ln(2 \pi) - \frac{1}{2} \ln |\Sigma_i| - (y_t - \mu_i) \Sigma_i^{-1} (y_t - \mu_i)^T\}
  \end{align}

  On obtient donc:

  \begin{align}
  \mu_i^{n + 1} = \frac{\sum_{t = 0}^{T - 1} q_t^i y_t}{\sum_{t = 0}^{T - 1}q_t^i}
  \end{align}

\item En maximisant par $\Sigma_i$: \\
  \begin{equation}
    \Sigma_i^{n + 1} = \frac{\sum_{t = 0}^{T - 1}q_t^i (y_t - \mu_i) (y_t - \mu_i)^T}{\sum_{t = 0}^{T - 1}q_t^i}
  \end{equation}
\end{itemize}


En réexprimant avec $\gamma(q_t) = p(q_t |y_0, y_1, \dots, y_T)$ et $\xi(q_t, q_{t + 1}) = p(q_t, q_{t + 1} | y)$:

\begin{System}
\pi_i^{n + 1} = \gamma_0 \\
\\
a_{ij}^{n + 1} = \frac{\sum_{t = 0}^{T - 2} \xi_{t, t + 1}^{i, j}}{\sum_{t = 0}^{T - 1} \gamma_t^i} \\
\\
\mu_i^{(n + 1)} =  \frac{\sum_{t = 0}^{T - 1} \gamma_t^i y_t}{\sum_{t = 0}^{T - 1}\gamma_t^i} \\
\\
\Sigma_i^{n + 1} = \frac{\sum_{t = 0}^{T - 1}\gamma_t^i (y_t - \mu_i) (y_t - \mu_i)^T}{\sum_{t = 0}^{T - 1}\gamma_t^i}

\end{System}
\end{document}
