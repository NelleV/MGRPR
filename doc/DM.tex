\documentclass{article}
%\usepackage[latin1]{inputenc}
\usepackage{graphicx,amssymb,amsmath,amsbsy} % extensions pour maths avancées
\usepackage{graphicx}           % extensions pour figures
\usepackage[T1]{fontenc}        % pour les charactères accentués 
\usepackage[utf8]{inputenc} 

\usepackage{stmaryrd} % Pour les crochets d'ensemble d'entier

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\argmax}{argmax}


\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.3in}
\setlength{\topmargin}{-0.4in}
\setlength{\topskip}{1in}    % between header and text
\setlength{\textheight}{8in} % height of main text
\setlength{\textwidth}{4.5in}    % width of text
\setlength{\oddsidemargin}{1in} % odd page left margin
\setlength{\evensidemargin}{1in} % even page left margin
%
%% Quelques raccourcis clavier :
\def\slantfrac#1#2{\kern.1em^{#1}\kern-.3em/\kern-.1em_{#2}}
\def\b#1{\mathbf{#1}}
\def\bs#1{\boldsymbol{#1}}
\def\m#1{\mathrm{#1}}
%
\newcommand{\greeksym}[1]{{\usefont{U}{psy}{m}{n}#1}}
\newcommand{\inc}{\mbox{\small\greeksym{d}\hskip 0.05ex}}%
\pagenumbering{arabic}
\date{\today}
\title{Modèles Graphiques}
\author{Barbara Gris \& Nelle Varoquaux}
\begin{document}
\maketitle
\tableofcontents{}
\vfill \eject


\section{Apprentissage dans les modèles discrets}
Soit $z$ et $x$ deux variables aléatoires pouvant prendre
respectivement $N$ et $K$ valeurs telles que: $p(z= m) =
\pi_m$ et $p(x=k|z=m) = \theta_{m,k}$

On suppose que l'on observe $n$ valeurs $(x_i, z_i)$ iid. On note 
$N_{i, j}$ le cardinal de 
$\{k \in \llbracket 1, n \rrbracket | (x_{k}, z_k) = (i, j)\}$ et $N_i$ celui
de $\{j \in \llbracket 1, n \rrbracket | z_j = i \}$.

L'estimateur $(\pi, \theta)$ du maximum de vraisemblance est:

$(\pi, \theta) = \argmax \{ \prod_{k=1}^n p(x_k, z_k) | \sum_{i=1}^{M}\pi_{i}=1; \forall m,\sum_{k=1}^{K}\theta_{m,k}=1 \}$

$(\pi, \theta) = \argmax \{ \prod_{i=1}^M \prod_{j=1}^K \pi_i^{N_ij} \theta_{i,j}^{N_{ij}} | \sum_{i=1}^{M}\pi_{i}=1; \forall m,\sum_{k=1}^{K}\theta_{m,k}=1 \}$

$(\pi, \theta) = \argmax \{\sum_{i=1}^{M} \sum_{j=1}^K N_{i,j}(\log\pi_{i} + \log(\theta_{i,j})| \sum_{i=1}^{M}\pi_{i}=1; \forall m,\sum_{k=1}^{K}\theta_{m,k}=1 \}$

$(\pi, \theta) = \argmax \{\sum_{i=1}^{M} \sum_{j=1}^K N_{i,j}\log\pi_{i} + \sum_{i=1}^{M} \sum_{j=1}^K n_{i,j}\log(\theta_{i,j}| \sum_{i=1}^{M}\pi_{i}=1; \forall m,\sum_{k=1}^{K}\theta_{m,k}=1 \}$

$(\pi, \theta) = \argmax \{\sum_{i=1}^{M} \sum_{j=1}^K N_{i,j}\log\pi_{i} + \sum_{i=1}^{M} \sum_{j=1}^K N_{i,j}\log\theta_{i,j} | \sum_{i=1}^{M}\pi_{i}=1; \forall m,\sum_{k=1}^{K}\theta_{m,k}=1 \}$

Comme la première double somme ne dépend que de $\pi$ et la deuxième que de $\theta$, on peut calculer les valeurs pour lesquelles elles sont maximales séparément, on a donc:
$$\pi = \argmax \{\sum_{i=1}^{M} \sum_{j=1}^K N_{i,j} \log\pi_i| \sum_{i=1}^{M}\pi_{i}=1 \}$$
$$\pi=\argmax \{\sum_{i=1}^{M}  N_{i} \log\pi_i| \sum_{i=1}^{M}\pi_{i}=1 \}$$
et
$$\theta = (\theta_1, ... \theta_M)$$

avec $\theta_i = \argmax \{ \sum_{j=1}^K N_{i,j}\log\theta_{i,j}|\forall m,\sum_{k=1}^{K}\theta_{m,k}=1 \}$



\subsection{Calcul du maximum de vraisemblance de $\pi$}
On utilise la méthode du lagrangien.\\

On note donc $\mathcal{L}(\pi, \lambda) = \sum_{i=1}^{M} (N_{i} \log \pi_{i}) + \lambda((\sum_{i=1}^{M}\pi_{i})-1)$\\

 $\mathcal{L}(\pi, \lambda)=\sum_{i=1}^{M} (N_{i} \log\pi_{i}+\lambda\pi_{i}+\frac{\lambda}{M})$.\\
 Comme le i-ème terme de la somme ne dépend que de $\pi_{i}$, pour maximiser
 $\mathcal{L}(\pi, \lambda)$ par rapport à $\pi_i$ il suffit de maximiser chaque i-ème terme de la somme par rapport à $\pi_i$. \\
Or $\frac{\partial}{\partial \pi_i}( N_{i} \log\pi_{i}+\lambda\pi_{i}+\frac{\lambda}{M}) = \frac{N_{i}}{\pi_i} + \lambda$ donc cette dérivée est nulle $ssi$ $\pi_i=-\frac{N_i}{\lambda}$, négative pour $\pi_i>-\frac{N_i}{\lambda}$ et positive sinon. 

Ainsi $frac{\partial N_{i}
\log\pi_{i}+\lambda\pi_{i}+\frac{\lambda}{M}}{\partial \pi_i}$ est maximal en $\pi_i=-\frac{N_i}{\lambda}$. Donc à $\lambda$ fixé, $\mathcal{L}(\pi, \lambda)$ est maximal pour $\hat{\pi}(\lambda)=(-\frac{N_1}{\lambda}, ..., -\frac{N_M}{\lambda})$.

De plus comme $\min\{\mathcal{L}(\hat{\pi}(\lambda),
\lambda)|\lambda\in\Re\} = \max \{\sum_{i=1}^{M} \sum_{j=1}^K N_{i,j} \log\pi_i| \sum_{i=1}^{M}\pi_{i}=1 \}$ et que ce minimum est atteint pour $\lambda $ tel que $\sum_{i=1}^{M}\hat{\pi}(\lambda)_i=1$, il est atteint pour $\lambda = -n$ et donc finalement l'estimateur du maximum de vraisemblance de $\pi$ est $$\hat{\pi}=(\frac{N_1}{n}, ..., \frac{N_M}{n}).$$


\subsection{Maximum de Vraisemblance de $\theta$}
De même que précédemment on peut calculer les valeurs de $\theta_{i}=(\theta_{i,1}, ..., \theta_{i,K})$ pour lesquelles $\sum_{j=1}^K N_{i,j}\log\theta_{i,j}$ est maximale avec $\forall m,\sum_{k=1}^{K}\theta_{m,k}=1$ séparément.
Un calcul analogue à celui de la partie précédente montre alors que $\hat{\theta}_{i}=(\frac{N_{i,1}}{n}, ..., \frac{N_{i,K}}{n})$

\section{Classification linéaire}
\subsection{1.Modèle génératif (LDA)}

On suppose que l'on observe $N$ valeurs $(x_i, y_i)$ iid. Calculons le maximum de vraisemblance de $\theta=(\pi, \Sigma, \mu_0, \mu_1)$.

La log-vraisemblance est par définition:

$l(\theta) = \log(\prod_{n=1}^N p (y_n | \pi)  p(x_{ n}|y_n, \theta))$

$l(\theta) = \sum_{n=1}^N \log p(y_n |\pi) + \sum_{n=1}^{N} \log p(x_{ n}|y_n,  \Sigma, \mu_0, \mu_1)$

On peut donc maximiser le premier terme indépendamment du deuxième terme.

La valeur de $\pi$ pour laquelle le premier terme est maximal a déjà été calculé dans l'exercice 1 : l'estimateur du maximum de vraisemblance de $\pi$ est $$\hat{\pi} = \frac{\sum_{n=1}^{N} y_{n}}{N}.$$

Le deuxième terme s'écrit:

$l(\theta) = \sum_{n=1}^{N} \log(\frac{1}{2\pi|\Sigma|^{1/2}} (\exp(-\frac{1}{2}(x_{n}-\mu_{1})^{T}\Sigma^{-1}(x_{n}-\mu_{1})))^{y_{n}}(\exp(-\frac{1}{2}(x_{n}-\mu_{0})^{T}\Sigma^{-1}(x_{n}-\mu_{0})))^{1-y_{n}}$ \\
$l(\theta) = \sum_{n=1}^{N} (\log(\frac{1}{2\pi|\Sigma|^{1/2}})-\frac{y_{n}}{2}(x_{n}-\mu_{1})^{T}\Sigma^{-1}(x_{n}-\mu_{1})-\frac{1-y_{n}}{2}(x_{n}-\mu_{0})^{T}\Sigma^{-1}(x_{n}-\mu_{0}))$.

\subsubsection{Estimation de $\mu_1$ et $\mu_0$}
$l( \Sigma, \mu_0, \mu_1) $ ne dépend de $ \mu_1$ que par $l_{1}=\sum_{n=1}^{N}-\frac{y_{n}}{2}(x_{n}-\mu_{1})^{T}\Sigma^{-1}(x_{n}-\mu_{1})$ donc la valeur de $ \mu_1$ pour laquelle cette quantité est maximale est également celle qui maximise $l( \Sigma, \mu_0, \mu_1) $ (par rapport à $\mu_1$).\\
Or $\nabla_{\mu_{1}}l_{1}=2\sum_{n=1}^{N}y_{n}(x_{n}+\mu_{1})^{T}\Sigma^{-1}$ donc ce gradient s'annule pour $\mu_{1}=\frac{\sum{n=1}^{N}y_{n}x_{n}}{\sum_{n=1}^{N}y_{n}}$.\\
Ainsi l'estimateur du maximum de vraisemblance de $\mu_{1}$ est $$\hat{\mu}_{1}=\frac{\sum_{n=1}^{N}y_{n}x_{n}}{\sum_{n=1}^{N}y_{n}}$$
et de même $$\hat{\mu}_{0}=\frac{\sum_{n=1}^{N}(1-y_{n})x_{n}}{\sum_{n=1}^{N}1-y_{n}}.$$

\subsubsection{Estimation de $\Sigma$}
Il faut maximiser $l( \Sigma, \hat{\mu_0}, \hat{\mu_1})$ par rapport à
$\Sigma$. On pose $\Gamma=\Sigma^{-1}$ et on maximise $l( \Gamma, \hat{\mu_0}, \hat{\mu_1}) $ par rapport à $\Gamma$.\\
Le gradient de $l$ en $\Gamma$ est $$\nabla_{\Gamma}l=\frac{1}{2} \sum_{n=1}^{N}(\Gamma^{-1}-y_{n}(x_{n}-\hat{\mu_{1}})(x_{n}-\hat{\mu_{1}})^{T}-(1-y_{n})(x_{n}-\hat{\mu_{0}})(x_{n}-\hat{\mu_{0}})^{T})$$
donc ce gradient s'annule pour $$\Gamma^{-1} = \frac{\sum_{n=1}^{N} ( y_{n}(x_{n}-\hat{\mu_{1}})(x_{n}-\hat{\mu_{1}})^{T}+(1-y_{n})(x_{n}-\hat{\mu_{0}})(x_{n}-\hat{\mu_{0}})^{T})}{N}.$$
Ainsi l'estimateur du maximum de vraisemblance de $\Sigma$ est $$\hat{\Sigma} = \frac{\sum_{n=1}^{N} ( y_{n}(x_{n}-\hat{\mu_{1}})(x_{n}-\hat{\mu_{1}})^{T}+(1-y_{n})(x_{n}-\hat{\mu_{0}})(x_{n}-\hat{\mu_{0}})^{T})}{N}.$$

\subsection{5.Modèle génératif (QDA)}
\subsubsection{maximum de vraisemblance }
On suppose que l'on observe $N$ valeurs $(x_i, y_i)$ iid. Calculons le maximum de vraisemblance de $\theta=(\pi, \Sigma_{0}, \Sigma_{1}, \mu_0, \mu_1)$.

La log-vraisemblance est par définition:

$l(\theta) = \log(\prod_{n=1}^N p (y_n | \pi)  p(x_{ n}|y_n, \theta))$

$l(\theta) = \sum_{n=1}^N \log(p(y_n |\pi)) + \sum_{n=1}^{N} \log(p(x_{ n}|y_n,  \Sigma_{0}, \Sigma_{1}, \mu_0, \mu_1))$

On peut donc maximiser le premier terme indépendamment du deuxième terme.

D'après l'exercice 1, l'estimateur du maximum de vraisemblance de $\pi$ est $$\hat{\pi} = \frac{\sum_{n=1}^{N} y_{n}}{N}.$$

Le deuxième terme s'écrit:

$l(\Sigma_{0}, \Sigma_{1}, \mu_0, \mu_1) = \sum_{n=1}^{N} \log(\frac{1}{2\pi}\frac{1}{|\Sigma_{1}|^{\frac{y_{n}}{2}}}\frac{1}{|\Sigma_{0}|^{\frac{1-y_{n}}{2}}} (\exp(-\frac{1}{2}(x_{n}-\mu_{1})^{T}\Sigma_{1}^{-1}(x_{n}-\mu_{1})))^{y_{n}}(\exp(-\frac{1}{2}(x_{n}-\mu_{0})^{T}\Sigma_{0}^{-1}(x_{n}-\mu_{0})))^{1-y_{n}}$ \\ \\
$l(\Sigma_{0}, \Sigma_{1}, \mu_0, \mu_1) = \sum_{n=1}^{N} (\log(\frac{1}{2\pi})-\frac{y_{n}}{2}\log(|\Sigma_{1}|)-\frac{1-y_{n}}{2}\log(|\Sigma_{0}|)-\frac{y_{n}}{2}(x_{n}-\mu_{1})^{T}\Sigma_{1}^{-1}(x_{n}-\mu_{1})-\frac{1-y_{n}}{2}(x_{n}-\mu_{0})^{T}\Sigma-{0}^{-1}(x_{n}-\mu_{0}))$.\\
Les estimateurs de $\mu_0$ et $\mu_1$ se calculent de la même manière qu'à la question 1.\\\\
$l(\Sigma_{0}, \Sigma_{1}, \mu_0, \mu_1)$ ne dépend de $\Sigma_{1}$ que par $l_{2}=\sum_{n=1}^{N}(-\frac{y_{n}}{2}\log(|\Sigma_{1}|))-\frac{y_{n}}{2}(x_{n}- \hat {\mu}_{1})^{T}\Sigma_{1}^{-1}(x_{n}-\hat {\mu}_{1})$. \\
On pose $\Gamma_{1} = \Sigma_1$ et on maximise $l_{2}$ en fonction de $\Gamma_{1}$. \\
$\nabla_{\Gamma_{1}}l_{2}= \frac{1}{2}(\sum_{n=1}^{N}y_{n})\Gamma^{-1}-\frac{1}{2}\sum_{n=1}^{N}(y_{n}(x_{n}-\mu_{1})(x_{n}-\mu_{1})^{T})$\\
donc ce gradient s'annule pour $\Gamma^{-1}=\frac{\sum_{n=1}^{N}(y_{n}(x_{n}-\mu_{1})(x_{n}-\mu_{1})^{T})}{\sum_{n=1}^{N}y_{n}}$.\\
Ainsi l'estimateur du maximum de vraisemblance de $\Sigma_{1}$ est 
$$\hat{\Sigma}_{1} = \frac{\sum_{n=1}^{N}(y_{n}(x_{n}-\mu_{1})(x_{n}-\mu_{1})^{T})}{\sum_{n=1}^{N}y_{n}}.$$
De même, on montre que $$\hat{\Sigma}_{0} = \frac{\sum_{n=1}^{N}((1-y_{n})(x_{n}-\mu_{0})(x_{n}-\mu_{0})^{T})}{\sum_{n=1}^{N}1-y_{n}}.$$

\subsubsection{conique d'équation $p(y=1|x)=0.5$}
D'après la formule de Bayes:\\
$p(y=1|x)=\frac{p(x|y=1)p(y=1)}{p(x)}$\\\\
$p(y=1|x)=\frac{\frac{\pi}{|\Sigma_{1}|}\exp(-\frac{1}{2}(x-\mu_{1})^{T}\Sigma_{1}^{-1}(x-\mu_{1}))}{\frac{\pi}{|\Sigma_{1}|}\exp(-\frac{1}{2}(x-\mu_{1})^{T}\Sigma_{1}^{-1}(x-\mu_{1})) + \frac{\pi}{|\Sigma_{0}|}\exp(-\frac{1}{2}(x-\mu_{0})^{T}\Sigma_{0}^{-1}(x-\mu_{0}))}$\\\\
$p(y=1|x)=\frac{1}{1+\exp(\log(\frac{1-\pi}{\pi})+\frac{1}{2}((x-\mu_{1})^{T}\Sigma_{1}^{-1}(x-\mu_{1}) - (x-\mu_{0})^{T}\Sigma_{0}^{-1}(x-\mu_{0})}$\\\\
Ainsi $p(y=1|x)=0.5$ $ssi$ $\log(\frac{1-\pi}{\pi})+\frac{1}{2}((x-\mu_{1})^{T}\Sigma_{1}^{-1}(x-\mu_{1}) - (x-\mu_{0})^{T}\Sigma_{0}^{-1}(x-\mu_{0}) = 0$ ce qui définit bien une conique.


\end{document}
